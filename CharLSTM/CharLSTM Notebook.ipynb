{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = open(\"../sherlock.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(set(dataset))\n",
    "len_words = len(words)\n",
    "max_length = 16\n",
    "length_seq = 100\n",
    "batch_size = 64\n",
    "training_test = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(words))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = []\n",
    "dataY = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_one_hot(sentence):\n",
    "    max_word_length = max_length\n",
    "    sent = []\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        word_encoding = np.zeros(shape=(max_word_length, len_words), dtype='float32')\n",
    "        \n",
    "        for i, char in enumerate(word):\n",
    "            try:\n",
    "                char_encoding = char_to_int[char]\n",
    "                one_hot = np.zeros(len_words, dtype='float32')\n",
    "                one_hot[char_encoding] = 1.0\n",
    "                word_encoding[i] = one_hot\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        \n",
    "        sent.append(np.array(word_encoding))\n",
    "    \n",
    "    return np.array(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(dataset) - length_seq, 1):\n",
    "    seq_in = dataset[i:i+length_seq]\n",
    "    seq_out = dataset[i+length_seq]\n",
    "    dataX.append(encode_one_hot(seq_in))\n",
    "    \n",
    "    out_encoding = np.zeros(shape=(len_words))\n",
    "    char_encoding = char_to_int[seq_out]\n",
    "    out_encoding[char_encoding] = 1\n",
    "    dataY.append(out_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_fillna(data):\n",
    "    lens = np.array([len(i) for i in data])\n",
    "    \n",
    "    mask = np.arange(lens.max()) < lens[:, None]\n",
    "    \n",
    "    out = np.zeros(shape=(mask.shape + (max_length, len_words)), dtype='float32')\n",
    "    \n",
    "    out[mask] = np.concatenate(data)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_ram(batch_size, init):\n",
    "    actDataX = []\n",
    "    actDataY = []\n",
    "    n_rows = batch_size\n",
    "    for i in range(init, init+batch_size, 1):\n",
    "        if i < len(dataX):\n",
    "            actDataX.append(dataX[i])\n",
    "            actDataY.append(dataY[i])\n",
    "            n_rows -= 1\n",
    "    if n_rows == 0:\n",
    "        actDataX = numpy_fillna(actDataX)\n",
    "        return True, actDataX, actDataY\n",
    "    else:\n",
    "        return False, actDataY, actDataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatch(batch_size, test=False):\n",
    "    if not test:\n",
    "        n_batch = int(len(dataX) * training_test // batch_size)\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            go, dx, dy = load_to_ram(batch_size, i * batch_size)\n",
    "            if go:\n",
    "                yield dx, dy\n",
    "    else:\n",
    "        n_batch_init = int(len(dataX) * training_test // batch_size)\n",
    "        n_batch_fin = int(len(dataX) // batch_size)\n",
    "        for i in range(n_batch_init, n_batch_fin, 1):\n",
    "            go, dx, dy = load_to_ram(batch_size, i * batch_size)\n",
    "            if go:\n",
    "                yield dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    input_size = shape[1]\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "    return tf.add(tf.matmul(input_, matrix, transpose_b=True), bias_term)\n",
    "\n",
    "\n",
    "def softmax(input_, out_dim, scope=None):\n",
    "    with tf.variable_scope(scope or \"Softmax\"):\n",
    "        W = tf.get_variable('W', [input_.get_shape()[1], out_dim])\n",
    "        b = tf.get_variable('b', [out_dim])\n",
    "    return tf.nn.softmax(tf.add(tf.matmul(input_, W), b))\n",
    "\n",
    "\n",
    "def conv2d(input_, output_dim, k_h, k_w, name=\"conv2d\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim])\n",
    "        b = tf.get_variable('b', [output_dim])\n",
    "    return tf.add(tf.nn.conv2d(input_, w, strides=[1, 1, 1, 1], padding='VALID'), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=2.0, activation=tf.nn.relu, scope='Highway'):\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = activation(linear(input_, size, scope=\"highway_lin_%d\" % idx))\n",
    "            t = tf.sigmoid(linear(input_, size, scope=\"highway_gate_%d\" % idx) + bias)\n",
    "            output = t * g + (1.0 - t) * input_\n",
    "            input_ = output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdnn(input_, kernels, kernel_features, scope=\"TDNN\"):\n",
    "    input_ = tf.reshape(input_, [-1, max_length, len_words])\n",
    "    input_ = tf.expand_dims(input_, 1)\n",
    "    layers = []\n",
    "    with tf.variable_scope(scope):\n",
    "        for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "            reduce_length = max_length - kernel_size + 1\n",
    "            conv = conv2d(input_, kernel_feature_size, 1, kernel_size, name=\"kernel_%d\" % kernel_size)\n",
    "            pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduce_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "            layers.append(tf.squeeze(pool, [1, 2]))\n",
    "        if len(kernels) > 1:\n",
    "            output = tf.concat(layers, 1)\n",
    "        else:\n",
    "            output = layers[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder('float32', shape=[None, None, max_length, len_words], name='X')\n",
    "Y = tf.placeholder('float32', shape=[None, len_words], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [1, 2, 3, 4, 5, 6, 7]\n",
    "kernel_features = [25, 50, 75, 100, 125, 150, 175]\n",
    "size = 700\n",
    "rnn_size = 650\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tdnn(X, kernels, kernel_features)\n",
    "cnn = highway(cnn, size)\n",
    "cnn = tf.reshape(cnn, [batch_size, -1, size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"LSTM\"):\n",
    "    def create_rnn_cell():\n",
    "        cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
    "        if dropout > 0.0:\n",
    "            cell = rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "        return cell\n",
    "    cell = create_rnn_cell()\n",
    "    initial_rnn_state = cell.zero_state(batch_size, dtype=\"float32\")\n",
    "    \n",
    "    outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn, initial_state=initial_rnn_state, dtype=tf.float32)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    last = outputs[-1]\n",
    "\n",
    "prediction = softmax(last, len_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # For simplicity I only use one epoch here, but you can change for any number\n",
    "learning_rate = 0.0001\n",
    "pred = prediction\n",
    "cost = - tf.reduce_sum(Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "predictor = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "acc = tf.reduce_mean(tf.cast(predictor, 'float32'))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_epoch: 1 batch: 1 loss: 276.936767578125 acc: 0.0\n",
      "act_epoch: 1 batch: 2 loss: 275.8157043457031 acc: 0.0\n",
      "act_epoch: 1 batch: 3 loss: 275.30810546875 acc: 0.015625\n",
      "act_epoch: 1 batch: 4 loss: 274.83326721191406 acc: 0.0234375\n",
      "act_epoch: 1 batch: 5 loss: 274.1716247558594 acc: 0.034375\n",
      "act_epoch: 1 batch: 6 loss: 274.1794738769531 acc: 0.033854166666666664\n",
      "act_epoch: 1 batch: 7 loss: 274.09073311941967 acc: 0.029017857142857144\n",
      "act_epoch: 1 batch: 8 loss: 273.6947250366211 acc: 0.04296875\n",
      "act_epoch: 1 batch: 9 loss: 272.74830457899304 acc: 0.04861111111111111\n",
      "act_epoch: 1 batch: 10 loss: 272.4758544921875 acc: 0.0609375\n",
      "act_epoch: 1 batch: 11 loss: 271.8074951171875 acc: 0.07102272727272728\n",
      "act_epoch: 1 batch: 12 loss: 270.8001963297526 acc: 0.08463541666666667\n",
      "act_epoch: 1 batch: 13 loss: 269.8661874624399 acc: 0.09375\n",
      "act_epoch: 1 batch: 14 loss: 268.9802812848772 acc: 0.09821428571428571\n",
      "act_epoch: 1 batch: 15 loss: 267.68515625 acc: 0.10520833333333333\n",
      "act_epoch: 1 batch: 16 loss: 266.4444990158081 acc: 0.109375\n",
      "act_epoch: 1 batch: 17 loss: 265.63200288660386 acc: 0.11121323529411764\n",
      "act_epoch: 1 batch: 18 loss: 264.50433858235675 acc: 0.11371527777777778\n",
      "act_epoch: 1 batch: 19 loss: 262.60341523822984 acc: 0.11759868421052631\n",
      "act_epoch: 1 batch: 20 loss: 260.8494071960449 acc: 0.12109375\n",
      "act_epoch: 1 batch: 21 loss: 259.5509025937035 acc: 0.12276785714285714\n",
      "act_epoch: 1 batch: 22 loss: 258.5547311956232 acc: 0.12357954545454546\n",
      "act_epoch: 1 batch: 23 loss: 257.6239007037619 acc: 0.125\n",
      "act_epoch: 1 batch: 24 loss: 256.5197124481201 acc: 0.126953125\n",
      "act_epoch: 1 batch: 25 loss: 255.20340270996093 acc: 0.12875\n",
      "act_epoch: 1 batch: 26 loss: 253.7719755906325 acc: 0.1310096153846154\n",
      "act_epoch: 1 batch: 27 loss: 252.70606090404368 acc: 0.13078703703703703\n",
      "act_epoch: 1 batch: 28 loss: 251.41626358032227 acc: 0.13169642857142858\n",
      "act_epoch: 1 batch: 29 loss: 250.24639261179956 acc: 0.1320043103448276\n",
      "act_epoch: 1 batch: 30 loss: 249.25509236653645 acc: 0.13020833333333334\n",
      "act_epoch: 1 batch: 31 loss: 248.2540086315524 acc: 0.1275201612903226\n",
      "act_epoch: 1 batch: 32 loss: 247.27380180358887 acc: 0.12646484375\n",
      "act_epoch: 1 batch: 33 loss: 246.02019061464253 acc: 0.12452651515151515\n",
      "act_epoch: 1 batch: 34 loss: 244.92355974982766 acc: 0.12270220588235294\n",
      "act_epoch: 1 batch: 35 loss: 243.99337594168526 acc: 0.12276785714285714\n",
      "act_epoch: 1 batch: 36 loss: 243.10348510742188 acc: 0.12239583333333333\n",
      "act_epoch: 1 batch: 37 loss: 242.30446769095755 acc: 0.12204391891891891\n",
      "act_epoch: 1 batch: 38 loss: 241.0683051661441 acc: 0.12335526315789473\n",
      "act_epoch: 1 batch: 39 loss: 240.09229493752505 acc: 0.1233974358974359\n",
      "act_epoch: 1 batch: 40 loss: 239.31981391906737 acc: 0.12421875\n",
      "act_epoch: 1 batch: 41 loss: 238.39104703577553 acc: 0.125\n",
      "act_epoch: 1 batch: 42 loss: 237.6607709612165 acc: 0.1253720238095238\n",
      "act_epoch: 1 batch: 43 loss: 236.65807040902072 acc: 0.12609011627906977\n",
      "act_epoch: 1 batch: 44 loss: 235.88368259776723 acc: 0.12748579545454544\n",
      "act_epoch: 1 batch: 45 loss: 235.1957784016927 acc: 0.1284722222222222\n",
      "act_epoch: 1 batch: 46 loss: 234.45158286716628 acc: 0.1280570652173913\n",
      "act_epoch: 1 batch: 47 loss: 233.68126337578957 acc: 0.1283244680851064\n",
      "act_epoch: 1 batch: 48 loss: 232.96895535786948 acc: 0.12890625\n",
      "act_epoch: 1 batch: 49 loss: 232.2336986308195 acc: 0.13010204081632654\n",
      "act_epoch: 1 batch: 50 loss: 231.49723480224608 acc: 0.13125\n",
      "act_epoch: 1 batch: 51 loss: 230.88213542863434 acc: 0.13143382352941177\n",
      "act_epoch: 1 batch: 52 loss: 230.17532436664288 acc: 0.13161057692307693\n",
      "act_epoch: 1 batch: 53 loss: 229.79514442299896 acc: 0.13148584905660377\n",
      "act_epoch: 1 batch: 54 loss: 229.23425264711733 acc: 0.13194444444444445\n",
      "act_epoch: 1 batch: 55 loss: 228.59241277521306 acc: 0.13295454545454546\n",
      "act_epoch: 1 batch: 56 loss: 228.19712993076868 acc: 0.13337053571428573\n",
      "act_epoch: 1 batch: 57 loss: 228.21401736610815 acc: 0.13404605263157895\n",
      "act_epoch: 1 batch: 58 loss: 227.77707303803544 acc: 0.13469827586206898\n",
      "act_epoch: 1 batch: 59 loss: 227.42994819253178 acc: 0.1347987288135593\n",
      "act_epoch: 1 batch: 60 loss: 226.90655746459962 acc: 0.13567708333333334\n",
      "act_epoch: 1 batch: 61 loss: 226.48408758444864 acc: 0.1362704918032787\n",
      "act_epoch: 1 batch: 62 loss: 226.00862884521484 acc: 0.13709677419354838\n",
      "act_epoch: 1 batch: 63 loss: 225.61546131921193 acc: 0.1371527777777778\n",
      "act_epoch: 1 batch: 64 loss: 225.23419404029846 acc: 0.13818359375\n",
      "act_epoch: 1 batch: 65 loss: 224.83243149977463 acc: 0.13822115384615385\n",
      "act_epoch: 1 batch: 66 loss: 224.35524333607066 acc: 0.1387310606060606\n",
      "act_epoch: 1 batch: 67 loss: 223.9661270824831 acc: 0.13922574626865672\n",
      "act_epoch: 1 batch: 68 loss: 223.52361320046816 acc: 0.1401654411764706\n",
      "act_epoch: 1 batch: 69 loss: 223.0396998308707 acc: 0.140625\n",
      "act_epoch: 1 batch: 70 loss: 222.6364994594029 acc: 0.14151785714285714\n",
      "act_epoch: 1 batch: 71 loss: 222.27947482256823 acc: 0.14194542253521128\n",
      "act_epoch: 1 batch: 72 loss: 222.1282115512424 acc: 0.142578125\n",
      "act_epoch: 1 batch: 73 loss: 221.73277554446705 acc: 0.14340753424657535\n",
      "act_epoch: 1 batch: 74 loss: 221.29266151222023 acc: 0.14358108108108109\n",
      "act_epoch: 1 batch: 75 loss: 220.81149780273438 acc: 0.14416666666666667\n",
      "act_epoch: 1 batch: 76 loss: 220.55497179533305 acc: 0.14391447368421054\n",
      "act_epoch: 1 batch: 77 loss: 220.6350386978744 acc: 0.14407467532467533\n",
      "act_epoch: 1 batch: 78 loss: 220.62922707582132 acc: 0.14463141025641027\n",
      "act_epoch: 1 batch: 79 loss: 220.4749896858312 acc: 0.14477848101265822\n",
      "act_epoch: 1 batch: 80 loss: 220.42516613006592 acc: 0.144921875\n",
      "act_epoch: 1 batch: 81 loss: 220.16904929832177 acc: 0.14506172839506173\n",
      "act_epoch: 1 batch: 82 loss: 220.07197682450456 acc: 0.14615091463414634\n",
      "act_epoch: 1 batch: 83 loss: 219.82087137612953 acc: 0.14627259036144577\n",
      "act_epoch: 1 batch: 84 loss: 219.8147290547689 acc: 0.14620535714285715\n",
      "act_epoch: 1 batch: 85 loss: 219.66047040153953 acc: 0.14650735294117648\n",
      "act_epoch: 1 batch: 86 loss: 219.33944950547328 acc: 0.14716569767441862\n",
      "act_epoch: 1 batch: 87 loss: 219.0799623686692 acc: 0.14780890804597702\n",
      "act_epoch: 1 batch: 88 loss: 218.86918570778587 acc: 0.14825994318181818\n",
      "act_epoch: 1 batch: 89 loss: 218.6905862186732 acc: 0.1483497191011236\n",
      "act_epoch: 1 batch: 90 loss: 218.50760142008463 acc: 0.14878472222222222\n",
      "act_epoch: 1 batch: 91 loss: 218.3646000453404 acc: 0.14852335164835165\n",
      "act_epoch: 1 batch: 92 loss: 218.17475874527642 acc: 0.14894701086956522\n",
      "act_epoch: 1 batch: 93 loss: 217.92801346317415 acc: 0.1490255376344086\n",
      "act_epoch: 1 batch: 94 loss: 217.77504502966048 acc: 0.14910239361702127\n",
      "act_epoch: 1 batch: 95 loss: 217.51958200555097 acc: 0.14917763157894737\n",
      "act_epoch: 1 batch: 96 loss: 217.20449829101562 acc: 0.14957682291666666\n",
      "act_epoch: 1 batch: 97 loss: 216.92302365647149 acc: 0.14996778350515463\n",
      "act_epoch: 1 batch: 98 loss: 216.73672345219825 acc: 0.14971301020408162\n",
      "act_epoch: 1 batch: 99 loss: 216.62292264687895 acc: 0.14977904040404041\n",
      "act_epoch: 1 batch: 100 loss: 216.5156771850586 acc: 0.14984375\n",
      "act_epoch: 1 batch: 101 loss: 216.33397168452197 acc: 0.1500618811881188\n",
      "act_epoch: 1 batch: 102 loss: 216.03086838067748 acc: 0.15042892156862744\n",
      "act_epoch: 1 batch: 103 loss: 215.81726103847467 acc: 0.1507888349514563\n",
      "act_epoch: 1 batch: 104 loss: 215.6489607004019 acc: 0.15099158653846154\n",
      "act_epoch: 1 batch: 105 loss: 215.46268266950335 acc: 0.15104166666666666\n",
      "act_epoch: 1 batch: 106 loss: 215.41460188379827 acc: 0.15109080188679244\n",
      "act_epoch: 1 batch: 107 loss: 215.20248926465757 acc: 0.15128504672897197\n",
      "act_epoch: 1 batch: 108 loss: 214.96485533537688 acc: 0.15133101851851852\n",
      "act_epoch: 1 batch: 109 loss: 214.90035422788847 acc: 0.15137614678899083\n",
      "act_epoch: 1 batch: 110 loss: 214.81595472856 acc: 0.1515625\n",
      "act_epoch: 1 batch: 111 loss: 214.64772501077738 acc: 0.15146396396396397\n",
      "act_epoch: 1 batch: 112 loss: 214.53092261723108 acc: 0.15150669642857142\n",
      "act_epoch: 1 batch: 113 loss: 214.44134724034672 acc: 0.1515486725663717\n",
      "act_epoch: 1 batch: 114 loss: 214.4923699362236 acc: 0.15145285087719298\n",
      "act_epoch: 1 batch: 115 loss: 214.57871876592222 acc: 0.15122282608695653\n",
      "act_epoch: 1 batch: 116 loss: 214.76632019569135 acc: 0.15126616379310345\n",
      "act_epoch: 1 batch: 117 loss: 214.6878429967114 acc: 0.1517094017094017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_epoch: 1 batch: 118 loss: 214.47042613918498 acc: 0.1517478813559322\n",
      "act_epoch: 1 batch: 119 loss: 214.36064904477416 acc: 0.15152310924369747\n",
      "act_epoch: 1 batch: 120 loss: 214.1375026702881 acc: 0.15182291666666667\n",
      "act_epoch: 1 batch: 121 loss: 213.99678342204447 acc: 0.1518595041322314\n",
      "act_epoch: 1 batch: 122 loss: 213.91456203773373 acc: 0.15176741803278687\n",
      "act_epoch: 1 batch: 123 loss: 213.81012732032838 acc: 0.1521849593495935\n",
      "act_epoch: 1 batch: 124 loss: 213.66326781242125 acc: 0.15209173387096775\n",
      "act_epoch: 1 batch: 125 loss: 213.6062724609375 acc: 0.152125\n",
      "act_epoch: 1 batch: 126 loss: 213.52723839169457 acc: 0.15215773809523808\n",
      "act_epoch: 1 batch: 127 loss: 213.31397854061578 acc: 0.15231299212598426\n",
      "act_epoch: 1 batch: 128 loss: 213.1184492111206 acc: 0.152587890625\n",
      "act_epoch: 1 batch: 129 loss: 212.94167452080305 acc: 0.1527374031007752\n",
      "act_epoch: 1 batch: 130 loss: 212.87886681189903 acc: 0.1527644230769231\n",
      "act_epoch: 1 batch: 131 loss: 212.75014466729783 acc: 0.15291030534351144\n",
      "act_epoch: 1 batch: 132 loss: 212.66202209935045 acc: 0.15317234848484848\n",
      "act_epoch: 1 batch: 133 loss: 212.56594940415002 acc: 0.15319548872180452\n",
      "act_epoch: 1 batch: 134 loss: 212.46994861204232 acc: 0.15345149253731344\n",
      "act_epoch: 1 batch: 135 loss: 212.3199314823857 acc: 0.15347222222222223\n",
      "act_epoch: 1 batch: 136 loss: 212.1227928610409 acc: 0.15349264705882354\n",
      "act_epoch: 1 batch: 137 loss: 212.12068454714588 acc: 0.15339872262773724\n",
      "act_epoch: 1 batch: 138 loss: 212.00144626783288 acc: 0.15353260869565216\n",
      "act_epoch: 1 batch: 139 loss: 211.92809334075707 acc: 0.15343974820143885\n",
      "act_epoch: 1 batch: 140 loss: 211.92432795933314 acc: 0.1533482142857143\n",
      "act_epoch: 1 batch: 141 loss: 211.91532540828624 acc: 0.15336879432624115\n",
      "act_epoch: 1 batch: 142 loss: 211.90016507430815 acc: 0.15338908450704225\n",
      "act_epoch: 1 batch: 143 loss: 211.78854594197307 acc: 0.15384615384615385\n",
      "act_epoch: 1 batch: 144 loss: 211.8087682723999 acc: 0.154296875\n",
      "act_epoch: 1 batch: 145 loss: 211.7096374511719 acc: 0.15463362068965517\n",
      "act_epoch: 1 batch: 146 loss: 211.6937201513003 acc: 0.1547517123287671\n",
      "act_epoch: 1 batch: 147 loss: 211.74163652277318 acc: 0.15476190476190477\n",
      "act_epoch: 1 batch: 148 loss: 211.83399963378906 acc: 0.15477195945945946\n",
      "act_epoch: 1 batch: 149 loss: 211.89611468219118 acc: 0.15467701342281878\n",
      "act_epoch: 1 batch: 150 loss: 212.02124318440755 acc: 0.1546875\n",
      "act_epoch: 1 batch: 151 loss: 212.00975875349235 acc: 0.1546978476821192\n",
      "act_epoch: 1 batch: 152 loss: 212.0925955521433 acc: 0.15460526315789475\n",
      "act_epoch: 1 batch: 153 loss: 212.14793366076898 acc: 0.15441176470588236\n",
      "act_epoch: 1 batch: 154 loss: 212.02909494375254 acc: 0.1544237012987013\n",
      "act_epoch: 1 batch: 155 loss: 212.0241946312689 acc: 0.15443548387096775\n",
      "act_epoch: 1 batch: 156 loss: 211.96267905602087 acc: 0.15454727564102563\n",
      "act_epoch: 1 batch: 157 loss: 211.91322375255027 acc: 0.1545581210191083\n",
      "act_epoch: 1 batch: 158 loss: 211.84355878226364 acc: 0.15476661392405064\n",
      "act_epoch: 1 batch: 159 loss: 211.83385136442365 acc: 0.15457940251572327\n",
      "act_epoch: 1 batch: 160 loss: 211.80256423950195 acc: 0.1546875\n",
      "act_epoch: 1 batch: 161 loss: 211.7243861796693 acc: 0.1547942546583851\n",
      "act_epoch: 1 batch: 162 loss: 211.63647046501254 acc: 0.15470679012345678\n",
      "act_epoch: 1 batch: 163 loss: 211.56008751992067 acc: 0.15481211656441718\n",
      "act_epoch: 1 batch: 164 loss: 211.45457691099585 acc: 0.15491615853658536\n",
      "act_epoch: 1 batch: 165 loss: 211.39904849890507 acc: 0.1550189393939394\n",
      "act_epoch: 1 batch: 166 loss: 211.2647126898708 acc: 0.15512048192771086\n",
      "act_epoch: 1 batch: 167 loss: 211.21624308146403 acc: 0.15522080838323354\n",
      "act_epoch: 1 batch: 168 loss: 211.2152102334159 acc: 0.15522693452380953\n",
      "act_epoch: 1 batch: 169 loss: 211.15613099950306 acc: 0.15532544378698224\n",
      "act_epoch: 1 batch: 170 loss: 211.11844437543084 acc: 0.1552389705882353\n",
      "act_epoch: 1 batch: 171 loss: 211.03660922580295 acc: 0.15533625730994152\n",
      "act_epoch: 1 batch: 172 loss: 211.04598227212594 acc: 0.15543241279069767\n",
      "act_epoch: 1 batch: 173 loss: 210.97166495791748 acc: 0.1554371387283237\n",
      "act_epoch: 1 batch: 174 loss: 211.02736935670347 acc: 0.1554418103448276\n",
      "act_epoch: 1 batch: 175 loss: 210.97340262276785 acc: 0.15571428571428572\n",
      "act_epoch: 1 batch: 176 loss: 210.89766190268776 acc: 0.15571732954545456\n",
      "act_epoch: 1 batch: 177 loss: 210.81640288789393 acc: 0.15589689265536724\n",
      "act_epoch: 1 batch: 178 loss: 210.71709973624584 acc: 0.15581109550561797\n",
      "act_epoch: 1 batch: 179 loss: 210.7055170496083 acc: 0.15563896648044692\n",
      "act_epoch: 1 batch: 180 loss: 210.6269891526964 acc: 0.1558159722222222\n",
      "act_epoch: 1 batch: 181 loss: 210.53819645834233 acc: 0.1559046961325967\n",
      "act_epoch: 1 batch: 182 loss: 210.48042473426233 acc: 0.15599244505494506\n",
      "act_epoch: 1 batch: 183 loss: 210.39857249442346 acc: 0.1560792349726776\n",
      "act_epoch: 1 batch: 184 loss: 210.31763201174527 acc: 0.15599524456521738\n",
      "act_epoch: 1 batch: 185 loss: 210.2202324119774 acc: 0.15608108108108107\n",
      "act_epoch: 1 batch: 186 loss: 210.14466045748802 acc: 0.15608198924731181\n",
      "act_epoch: 1 batch: 187 loss: 210.10862438181505 acc: 0.15599933155080214\n",
      "act_epoch: 1 batch: 188 loss: 210.01430641336643 acc: 0.15608377659574468\n",
      "act_epoch: 1 batch: 189 loss: 209.98120827649637 acc: 0.15591931216931218\n",
      "act_epoch: 1 batch: 190 loss: 209.92852486058285 acc: 0.15583881578947367\n",
      "act_epoch: 1 batch: 191 loss: 209.81404672992167 acc: 0.15600458115183247\n",
      "act_epoch: 1 batch: 192 loss: 209.78984673817953 acc: 0.15592447916666666\n",
      "act_epoch: 1 batch: 193 loss: 209.73634385815555 acc: 0.1559261658031088\n",
      "act_epoch: 1 batch: 194 loss: 209.61070015504188 acc: 0.1561694587628866\n",
      "act_epoch: 1 batch: 195 loss: 209.48621935722156 acc: 0.1563301282051282\n",
      "act_epoch: 1 batch: 196 loss: 209.4685341971261 acc: 0.15625\n",
      "act_epoch: 1 batch: 197 loss: 209.44228545058198 acc: 0.15632931472081218\n",
      "act_epoch: 1 batch: 198 loss: 209.3776144933219 acc: 0.15632891414141414\n",
      "act_epoch: 1 batch: 199 loss: 209.33358918003103 acc: 0.1565640703517588\n",
      "act_epoch: 1 batch: 200 loss: 209.28766784667968 acc: 0.156640625\n",
      "act_epoch: 1 batch: 201 loss: 209.30206579711307 acc: 0.1566386815920398\n",
      "act_epoch: 1 batch: 202 loss: 209.24795751288386 acc: 0.1567914603960396\n",
      "act_epoch: 1 batch: 203 loss: 209.2642262275583 acc: 0.15686576354679804\n",
      "act_epoch: 1 batch: 204 loss: 209.2271994048474 acc: 0.15678615196078433\n",
      "act_epoch: 1 batch: 205 loss: 209.14166445848417 acc: 0.15693597560975608\n",
      "act_epoch: 1 batch: 206 loss: 209.0942056155899 acc: 0.15700849514563106\n",
      "act_epoch: 1 batch: 207 loss: 209.05930221483902 acc: 0.1570048309178744\n",
      "act_epoch: 1 batch: 208 loss: 209.00693225860596 acc: 0.1572265625\n",
      "act_epoch: 1 batch: 209 loss: 208.96640153364703 acc: 0.1572218899521531\n",
      "act_epoch: 1 batch: 210 loss: 208.94606112525577 acc: 0.15729166666666666\n",
      "act_epoch: 1 batch: 211 loss: 208.9504954984403 acc: 0.15736078199052134\n",
      "act_epoch: 1 batch: 212 loss: 208.84944879783774 acc: 0.15742924528301888\n",
      "act_epoch: 1 batch: 213 loss: 208.74559823336176 acc: 0.15771713615023475\n",
      "act_epoch: 1 batch: 214 loss: 208.68592342483663 acc: 0.15778329439252337\n",
      "act_epoch: 1 batch: 215 loss: 208.79602270791696 acc: 0.15755813953488373\n",
      "act_epoch: 1 batch: 216 loss: 208.7563779619005 acc: 0.15747974537037038\n",
      "act_epoch: 1 batch: 217 loss: 208.67894333518595 acc: 0.1576900921658986\n",
      "act_epoch: 1 batch: 218 loss: 208.6212973638412 acc: 0.1577551605504587\n",
      "act_epoch: 1 batch: 219 loss: 208.6099696050496 acc: 0.1578909817351598\n",
      "act_epoch: 1 batch: 220 loss: 208.59935233376243 acc: 0.15774147727272728\n",
      "act_epoch: 1 batch: 221 loss: 208.51610485784607 acc: 0.15787613122171945\n",
      "act_epoch: 1 batch: 222 loss: 208.46259967700854 acc: 0.15779842342342343\n",
      "act_epoch: 1 batch: 223 loss: 208.41881947880904 acc: 0.15772141255605382\n",
      "act_epoch: 1 batch: 224 loss: 208.41521855763025 acc: 0.15771484375\n",
      "act_epoch: 1 batch: 225 loss: 208.37207661946616 acc: 0.15756944444444446\n",
      "act_epoch: 1 batch: 226 loss: 208.34641063318844 acc: 0.15749446902654868\n",
      "act_epoch: 1 batch: 227 loss: 208.2689632466186 acc: 0.15762665198237885\n",
      "act_epoch: 1 batch: 228 loss: 208.1674166896887 acc: 0.15762061403508773\n",
      "act_epoch: 1 batch: 229 loss: 208.13030905910975 acc: 0.15754639737991266\n",
      "act_epoch: 1 batch: 230 loss: 208.1389865377675 acc: 0.15740489130434782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_epoch: 1 batch: 231 loss: 208.08402526327026 acc: 0.15739989177489178\n",
      "act_epoch: 1 batch: 232 loss: 208.0532023988921 acc: 0.15746228448275862\n",
      "act_epoch: 1 batch: 233 loss: 207.99548418429788 acc: 0.15745708154506438\n",
      "act_epoch: 1 batch: 234 loss: 207.91334859733908 acc: 0.15751869658119658\n",
      "act_epoch: 1 batch: 235 loss: 207.9443416514295 acc: 0.15764627659574468\n",
      "act_epoch: 1 batch: 236 loss: 207.98007441375216 acc: 0.15750794491525424\n",
      "act_epoch: 1 batch: 237 loss: 207.97498324752357 acc: 0.15763449367088608\n",
      "act_epoch: 1 batch: 238 loss: 208.03111183743516 acc: 0.15743172268907563\n",
      "act_epoch: 1 batch: 239 loss: 208.087757381934 acc: 0.15723064853556484\n",
      "act_epoch: 1 batch: 240 loss: 208.02350686391193 acc: 0.15709635416666667\n",
      "act_epoch: 1 batch: 241 loss: 207.96015100756128 acc: 0.15715767634854771\n",
      "act_epoch: 1 batch: 242 loss: 207.886010288207 acc: 0.1572184917355372\n",
      "act_epoch: 1 batch: 243 loss: 207.82083789213203 acc: 0.15727880658436214\n",
      "act_epoch: 1 batch: 244 loss: 207.76221172145156 acc: 0.15740266393442623\n",
      "act_epoch: 1 batch: 245 loss: 207.7626495361328 acc: 0.15727040816326532\n",
      "act_epoch: 1 batch: 246 loss: 207.74165921094942 acc: 0.15720274390243902\n",
      "act_epoch: 1 batch: 247 loss: 207.72449403832317 acc: 0.1571988866396761\n",
      "act_epoch: 1 batch: 248 loss: 207.73183213510822 acc: 0.15719506048387097\n",
      "act_epoch: 1 batch: 249 loss: 207.70637009708756 acc: 0.15719126506024098\n",
      "act_epoch: 1 batch: 250 loss: 207.72878497314454 acc: 0.1571875\n",
      "act_epoch: 1 batch: 251 loss: 207.6733206334817 acc: 0.15724601593625498\n",
      "act_epoch: 1 batch: 252 loss: 207.61029210166325 acc: 0.1572420634920635\n",
      "act_epoch: 1 batch: 253 loss: 207.55085501086572 acc: 0.1573616600790514\n",
      "act_epoch: 1 batch: 254 loss: 207.50091642845334 acc: 0.15735728346456693\n",
      "act_epoch: 1 batch: 255 loss: 207.47178937126608 acc: 0.1574142156862745\n",
      "act_epoch: 1 batch: 256 loss: 207.43009459972382 acc: 0.15728759765625\n",
      "act_epoch: 1 batch: 257 loss: 207.4782871587731 acc: 0.15740515564202334\n",
      "act_epoch: 1 batch: 258 loss: 207.66007209008978 acc: 0.15727955426356588\n",
      "act_epoch: 1 batch: 259 loss: 207.75848253169116 acc: 0.1570945945945946\n",
      "act_epoch: 1 batch: 260 loss: 207.80624759380635 acc: 0.15697115384615384\n",
      "act_epoch: 1 batch: 261 loss: 207.74551473449472 acc: 0.15690852490421456\n",
      "act_epoch: 1 batch: 262 loss: 207.7405567897185 acc: 0.15684637404580154\n",
      "act_epoch: 1 batch: 263 loss: 207.79867478287267 acc: 0.15666587452471484\n",
      "act_epoch: 1 batch: 264 loss: 207.8407680049087 acc: 0.15654592803030304\n",
      "act_epoch: 1 batch: 265 loss: 207.81744851166349 acc: 0.1564858490566038\n",
      "act_epoch: 1 batch: 266 loss: 207.77186670518458 acc: 0.15642622180451127\n",
      "act_epoch: 1 batch: 267 loss: 207.83605288387685 acc: 0.15625\n",
      "act_epoch: 1 batch: 268 loss: 207.9127700008563 acc: 0.15584188432835822\n",
      "act_epoch: 1 batch: 269 loss: 207.960673505932 acc: 0.1556110594795539\n",
      "act_epoch: 1 batch: 270 loss: 207.96863318549262 acc: 0.15561342592592592\n",
      "act_epoch: 1 batch: 271 loss: 207.98475489317272 acc: 0.155385147601476\n",
      "act_epoch: 1 batch: 272 loss: 208.00605285868926 acc: 0.15544577205882354\n",
      "act_epoch: 1 batch: 273 loss: 208.00120505308493 acc: 0.1553342490842491\n",
      "act_epoch: 1 batch: 274 loss: 208.03086530031078 acc: 0.1553375912408759\n",
      "act_epoch: 1 batch: 275 loss: 208.0562537175959 acc: 0.15517045454545456\n",
      "act_epoch: 1 batch: 276 loss: 208.04392203731814 acc: 0.155174365942029\n",
      "act_epoch: 1 batch: 277 loss: 208.01496821103973 acc: 0.15517824909747294\n",
      "act_epoch: 1 batch: 278 loss: 208.03040225900335 acc: 0.15495728417266186\n",
      "act_epoch: 1 batch: 279 loss: 208.01382446289062 acc: 0.15507392473118278\n",
      "act_epoch: 1 batch: 280 loss: 208.00840939113073 acc: 0.15496651785714285\n",
      "act_epoch: 1 batch: 281 loss: 207.9829927491972 acc: 0.15497108540925267\n",
      "act_epoch: 1 batch: 282 loss: 207.98205772021137 acc: 0.15497562056737588\n",
      "act_epoch: 1 batch: 283 loss: 208.029024912696 acc: 0.15481448763250882\n",
      "act_epoch: 1 batch: 284 loss: 208.0553682891416 acc: 0.15481954225352113\n",
      "act_epoch: 1 batch: 285 loss: 208.04971757855333 acc: 0.15487938596491227\n",
      "act_epoch: 1 batch: 286 loss: 208.0087267468859 acc: 0.15504807692307693\n",
      "act_epoch: 1 batch: 287 loss: 208.00734293336237 acc: 0.15494337979094078\n",
      "act_epoch: 1 batch: 288 loss: 207.96944035424127 acc: 0.1552191840277778\n",
      "act_epoch: 1 batch: 289 loss: 207.9101066193366 acc: 0.15543901384083045\n",
      "act_epoch: 1 batch: 290 loss: 207.87614651384024 acc: 0.1554956896551724\n",
      "act_epoch: 1 batch: 291 loss: 207.82407172029372 acc: 0.15555197594501718\n",
      "act_epoch: 1 batch: 292 loss: 207.7691308635555 acc: 0.15560787671232876\n",
      "act_epoch: 1 batch: 293 loss: 207.7712640859975 acc: 0.15555674061433447\n",
      "act_epoch: 1 batch: 294 loss: 207.7255218402058 acc: 0.1556122448979592\n",
      "act_epoch: 1 batch: 295 loss: 207.72124825170485 acc: 0.155614406779661\n",
      "act_epoch: 1 batch: 296 loss: 207.69941164996172 acc: 0.15572212837837837\n",
      "act_epoch: 1 batch: 297 loss: 207.64506145438762 acc: 0.15577651515151514\n",
      "act_epoch: 1 batch: 298 loss: 207.66498586155424 acc: 0.15577810402684564\n",
      "act_epoch: 1 batch: 299 loss: 207.64082275186493 acc: 0.15588419732441472\n",
      "act_epoch: 1 batch: 300 loss: 207.62996831258138 acc: 0.15598958333333332\n",
      "act_epoch: 1 batch: 301 loss: 207.6373710252122 acc: 0.1559904485049834\n",
      "act_epoch: 1 batch: 302 loss: 207.67450729269066 acc: 0.15578435430463577\n",
      "act_epoch: 1 batch: 303 loss: 207.65113603950726 acc: 0.15594059405940594\n",
      "act_epoch: 1 batch: 304 loss: 207.6673241163555 acc: 0.15578741776315788\n",
      "act_epoch: 1 batch: 305 loss: 207.63081790111104 acc: 0.15589139344262296\n",
      "act_epoch: 1 batch: 306 loss: 207.57042858647364 acc: 0.1559436274509804\n",
      "act_epoch: 1 batch: 307 loss: 207.5626739601359 acc: 0.1558428338762215\n",
      "act_epoch: 1 batch: 308 loss: 207.53367743554054 acc: 0.15584415584415584\n",
      "act_epoch: 1 batch: 309 loss: 207.587302248069 acc: 0.15574433656957928\n",
      "act_epoch: 1 batch: 310 loss: 207.59682976507372 acc: 0.15574596774193547\n",
      "act_epoch: 1 batch: 311 loss: 207.60696205065565 acc: 0.15564710610932475\n",
      "act_epoch: 1 batch: 312 loss: 207.5901628151918 acc: 0.15564903846153846\n",
      "act_epoch: 1 batch: 313 loss: 207.57676521276895 acc: 0.1557008785942492\n",
      "act_epoch: 1 batch: 314 loss: 207.5331183269525 acc: 0.15575238853503184\n",
      "act_epoch: 1 batch: 315 loss: 207.50736534481956 acc: 0.15575396825396826\n",
      "act_epoch: 1 batch: 316 loss: 207.4904142935065 acc: 0.15575553797468356\n",
      "act_epoch: 1 batch: 317 loss: 207.50034228785182 acc: 0.1557570977917981\n",
      "act_epoch: 1 batch: 318 loss: 207.44079709802784 acc: 0.1558077830188679\n",
      "act_epoch: 1 batch: 319 loss: 207.41556781063258 acc: 0.15580916927899688\n",
      "act_epoch: 1 batch: 320 loss: 207.38023734092712 acc: 0.155810546875\n",
      "act_epoch: 1 batch: 321 loss: 207.357489897826 acc: 0.15581191588785046\n",
      "act_epoch: 1 batch: 322 loss: 207.3152435018409 acc: 0.1559103260869565\n",
      "act_epoch: 1 batch: 323 loss: 207.27634484258598 acc: 0.15591137770897834\n",
      "act_epoch: 1 batch: 324 loss: 207.21671299875518 acc: 0.15591242283950618\n",
      "act_epoch: 1 batch: 325 loss: 207.17486285869893 acc: 0.15596153846153846\n",
      "act_epoch: 1 batch: 326 loss: 207.11881335673888 acc: 0.1560103527607362\n",
      "act_epoch: 1 batch: 327 loss: 207.07488015084456 acc: 0.15601108562691132\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iterator_minibatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e2ab21ba8a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mbatch_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0miterator_test\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mdata_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_out\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iterator_minibatch' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    best_acc = 0.0\n",
    "    act_epoch = 0\n",
    "    \n",
    "    while act_epoch < epochs:\n",
    "        loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        batch = 1\n",
    "        act_epoch += 1\n",
    "        for iterator_train in iterate_minibatch(batch_size):\n",
    "            data_in, data_out = iterator_train\n",
    "            _, c, a = sess.run([optimizer, cost, acc], feed_dict={X: data_in, Y: data_out})\n",
    "            loss += c\n",
    "            accuracy += a\n",
    "            print(\"act_epoch:\", act_epoch, \"batch:\", batch, \"loss:\", loss/batch, \"acc:\", accuracy/batch)\n",
    "            batch += 1\n",
    "        \n",
    "        if act_epoch == 1 or act_epoch % 10 == 0:\n",
    "            accuracy_test = 0.0\n",
    "            batch_test = 0.0\n",
    "            for iterator_test in iterate_minibatch(batch_size, test=True):\n",
    "                data_in, data_out = iterator_test\n",
    "                a = sess.run(acc, feed_dict={X: data_in, Y: data_out})\n",
    "                accuracy_test += a\n",
    "                batch_test += 1\n",
    "            print(\"Accuracy Test:\", accuracy_test/batch_test)\n",
    "        \n",
    "        path = saver.save(sess, \"./tmp/model\")\n",
    "        print(\"Saving in Epoch\", act_epoch, \"in path\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
